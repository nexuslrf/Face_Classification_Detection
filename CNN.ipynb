{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Here I use [Pytorch](https://pytorch.org) as the NN framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from FDDB_dataloader import FDDB\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel stat values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([T.Resize((96,96)), T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = FDDB('train_list.npy', transform=transforms)\n",
    "db_val = FDDB('val_list.npy', transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6139, 0.4767, 0.4134])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_train[0][0].mean([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "std = torch.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20680/20680 [00:25<00:00, 800.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5116, 0.4200, 0.3651])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for img, label in tqdm.tqdm(db_train):\n",
    "    mean += img.mean([1,2])\n",
    "mean /= len(db_train)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20680/20680 [00:26<00:00, 784.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2655, 0.2430, 0.2420])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = mean.reshape([3,1,1])\n",
    "\n",
    "for img, label in tqdm.tqdm(db_train):\n",
    "    std += ((img - tmp)**2).mean([1,2])\n",
    "std /= len(db_train)\n",
    "std = std.sqrt()\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del db_train, db_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "workers = 4\n",
    "gpu = 1\n",
    "arch = 'resnet18'\n",
    "optim = 'Adam'\n",
    "lr = 0.1\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "epochs = 200\n",
    "save_epoch = 50\n",
    "save = False\n",
    "suffix = ''\n",
    "print_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pytorch MultiThread DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = T.Normalize(mean = [0.5116, 0.4200, 0.3651],\n",
    "                         std = [0.2655, 0.2430, 0.2420])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = FDDB('train_list.npy', \n",
    "                transform=T.Compose([\n",
    "                    T.Resize((96,96)),\n",
    "                    T.RandomHorizontalFlip(),\n",
    "                    T.ToTensor(),\n",
    "                    normalize\n",
    "                ]),\n",
    "                zero_one=True\n",
    "               )\n",
    "db_val = FDDB('val_list.npy', \n",
    "              transform=T.Compose([\n",
    "                  T.Resize((96,96)),\n",
    "                  T.ToTensor(),\n",
    "                  normalize\n",
    "              ]),\n",
    "              zero_one=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(db_train, batch_size=batch_size, shuffle=True,\n",
    "                         num_workers=workers, pin_memory=True)\n",
    "val_loader = DataLoader(db_val, batch_size=batch_size, shuffle=False,\n",
    "                       num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.__dict__[arch](num_classes=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "if optim == 'SGD':\n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), lr, momentum = momentum, weight_decay=weight_decay)\n",
    "elif optim == 'Adam':\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top0 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if gpu is not None:\n",
    "            input = input.cuda(gpu, non_blocking=True)\n",
    "            target = target.float().unsqueeze(-1).cuda(gpu, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        acc = accuracy_VOC2012(output, target)\n",
    "        \n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top0.update(acc[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Acc@0 {top0.val:.3f} ({top0.avg:.3f})\\t'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses, top0=top0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top0 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            if gpu is not None:\n",
    "                input = input.cuda(gpu, non_blocking=True)\n",
    "                target = target.float().unsqueeze(-1).cuda(gpu, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            acc = accuracy_VOC2012(output, target)\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top0.update(acc[0], input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if i % print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Acc@0 {top0.val:.3f} ({top0.avg:.3f})\\t'.format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top0=top0))\n",
    "\n",
    "        print(' * Acc@0 {top0.avg:.3f}'.format(top0=top0))\n",
    "    return top0.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best_{}_{}.pth.tar'.format(arch, suffix))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def accuracy_VOC2012(output, target):\n",
    "    with torch.no_grad():\n",
    "        batch_size = target.size(0)\n",
    "        accur = output.gt(0.).long().eq(target.long()).float().mean()\n",
    "        res = []\n",
    "        res.append(accur)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.cuda(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/81]\tTime 4.928 (4.928)\tData 4.185 (4.185)\tLoss 0.6603 (0.6603)\tAcc@0 0.703 (0.703)\t\n",
      "Epoch: [0][10/81]\tTime 0.332 (0.749)\tData 0.000 (0.403)\tLoss 0.5184 (0.9072)\tAcc@0 0.789 (0.793)\t\n",
      "Epoch: [0][20/81]\tTime 0.340 (0.553)\tData 0.000 (0.221)\tLoss 0.4189 (0.7607)\tAcc@0 0.773 (0.791)\t\n",
      "Epoch: [0][30/81]\tTime 0.331 (0.482)\tData 0.000 (0.156)\tLoss 0.3741 (0.6621)\tAcc@0 0.828 (0.796)\t\n",
      "Epoch: [0][40/81]\tTime 0.331 (0.446)\tData 0.000 (0.123)\tLoss 0.4104 (0.6058)\tAcc@0 0.820 (0.798)\t\n",
      "Epoch: [0][50/81]\tTime 0.344 (0.424)\tData 0.000 (0.103)\tLoss 0.4514 (0.5787)\tAcc@0 0.770 (0.799)\t\n",
      "Epoch: [0][60/81]\tTime 0.334 (0.410)\tData 0.000 (0.089)\tLoss 0.3978 (0.5466)\tAcc@0 0.832 (0.806)\t\n",
      "Epoch: [0][70/81]\tTime 0.330 (0.399)\tData 0.000 (0.080)\tLoss 0.3615 (0.5234)\tAcc@0 0.867 (0.814)\t\n",
      "Epoch: [0][80/81]\tTime 0.311 (0.391)\tData 0.000 (0.072)\tLoss 0.2387 (0.4952)\tAcc@0 0.930 (0.825)\t\n",
      "Test: [0/37]\tTime 1.332 (1.332)\tLoss 0.5973 (0.5973)\tAcc@0 0.668 (0.668)\t\n",
      "Test: [10/37]\tTime 0.108 (0.279)\tLoss 0.6848 (0.6455)\tAcc@0 0.559 (0.615)\t\n",
      "Test: [20/37]\tTime 0.308 (0.238)\tLoss 0.6501 (0.6546)\tAcc@0 0.621 (0.598)\t\n",
      "Test: [30/37]\tTime 0.109 (0.221)\tLoss 0.5822 (0.6588)\tAcc@0 0.656 (0.586)\t\n",
      " * Acc@0 0.598\n",
      "Epoch: [1][0/81]\tTime 1.286 (1.286)\tData 1.142 (1.142)\tLoss 0.2795 (0.2795)\tAcc@0 0.895 (0.895)\t\n",
      "Epoch: [1][10/81]\tTime 0.334 (0.426)\tData 0.000 (0.123)\tLoss 0.2263 (0.2666)\tAcc@0 0.902 (0.899)\t\n",
      "Epoch: [1][20/81]\tTime 0.328 (0.386)\tData 0.000 (0.074)\tLoss 0.1624 (0.2706)\tAcc@0 0.945 (0.899)\t\n",
      "Epoch: [1][30/81]\tTime 0.333 (0.371)\tData 0.000 (0.057)\tLoss 0.2473 (0.2607)\tAcc@0 0.910 (0.903)\t\n",
      "Epoch: [1][40/81]\tTime 0.325 (0.363)\tData 0.000 (0.048)\tLoss 0.2394 (0.2613)\tAcc@0 0.895 (0.901)\t\n",
      "Epoch: [1][50/81]\tTime 0.342 (0.359)\tData 0.000 (0.043)\tLoss 0.2209 (0.2602)\tAcc@0 0.930 (0.902)\t\n",
      "Epoch: [1][60/81]\tTime 0.332 (0.356)\tData 0.000 (0.039)\tLoss 0.2572 (0.2546)\tAcc@0 0.922 (0.904)\t\n",
      "Epoch: [1][70/81]\tTime 0.378 (0.355)\tData 0.000 (0.036)\tLoss 0.1512 (0.2476)\tAcc@0 0.953 (0.907)\t\n",
      "Epoch: [1][80/81]\tTime 0.322 (0.352)\tData 0.000 (0.034)\tLoss 0.2534 (0.2428)\tAcc@0 0.880 (0.909)\t\n",
      "Test: [0/37]\tTime 1.276 (1.276)\tLoss 0.3374 (0.3374)\tAcc@0 0.840 (0.840)\t\n",
      "Test: [10/37]\tTime 0.116 (0.295)\tLoss 0.6097 (0.4993)\tAcc@0 0.688 (0.744)\t\n",
      "Test: [20/37]\tTime 0.140 (0.242)\tLoss 0.5868 (0.5531)\tAcc@0 0.715 (0.706)\t\n",
      "Test: [30/37]\tTime 0.108 (0.215)\tLoss 0.5141 (0.5606)\tAcc@0 0.758 (0.701)\t\n",
      " * Acc@0 0.708\n",
      "Epoch: [2][0/81]\tTime 1.598 (1.598)\tData 1.472 (1.472)\tLoss 0.1694 (0.1694)\tAcc@0 0.945 (0.945)\t\n",
      "Epoch: [2][10/81]\tTime 0.347 (0.454)\tData 0.000 (0.153)\tLoss 0.1811 (0.1902)\tAcc@0 0.938 (0.928)\t\n",
      "Epoch: [2][20/81]\tTime 0.341 (0.400)\tData 0.000 (0.090)\tLoss 0.2396 (0.1973)\tAcc@0 0.906 (0.926)\t\n",
      "Epoch: [2][30/81]\tTime 0.340 (0.380)\tData 0.000 (0.068)\tLoss 0.1601 (0.1933)\tAcc@0 0.938 (0.928)\t\n",
      "Epoch: [2][40/81]\tTime 0.332 (0.371)\tData 0.000 (0.056)\tLoss 0.2175 (0.1953)\tAcc@0 0.922 (0.928)\t\n",
      "Epoch: [2][50/81]\tTime 0.349 (0.365)\tData 0.000 (0.049)\tLoss 0.2291 (0.1976)\tAcc@0 0.930 (0.927)\t\n",
      "Epoch: [2][60/81]\tTime 0.334 (0.361)\tData 0.000 (0.045)\tLoss 0.1666 (0.1940)\tAcc@0 0.949 (0.929)\t\n",
      "Epoch: [2][70/81]\tTime 0.337 (0.359)\tData 0.000 (0.041)\tLoss 0.2352 (0.1937)\tAcc@0 0.922 (0.930)\t\n",
      "Epoch: [2][80/81]\tTime 0.323 (0.356)\tData 0.000 (0.039)\tLoss 0.1927 (0.1927)\tAcc@0 0.930 (0.930)\t\n",
      "Test: [0/37]\tTime 1.219 (1.219)\tLoss 0.4521 (0.4521)\tAcc@0 0.789 (0.789)\t\n",
      "Test: [10/37]\tTime 0.166 (0.274)\tLoss 0.2862 (0.3737)\tAcc@0 0.871 (0.847)\t\n",
      "Test: [20/37]\tTime 0.241 (0.228)\tLoss 0.2650 (0.3261)\tAcc@0 0.906 (0.871)\t\n",
      "Test: [30/37]\tTime 0.228 (0.212)\tLoss 0.2136 (0.3055)\tAcc@0 0.930 (0.883)\t\n",
      " * Acc@0 0.887\n",
      "Epoch: [3][0/81]\tTime 1.654 (1.654)\tData 1.518 (1.518)\tLoss 0.1632 (0.1632)\tAcc@0 0.949 (0.949)\t\n",
      "Epoch: [3][10/81]\tTime 0.317 (0.460)\tData 0.000 (0.156)\tLoss 0.1081 (0.1582)\tAcc@0 0.957 (0.942)\t\n",
      "Epoch: [3][20/81]\tTime 0.337 (0.403)\tData 0.000 (0.092)\tLoss 0.1687 (0.1573)\tAcc@0 0.941 (0.944)\t\n",
      "Epoch: [3][30/81]\tTime 0.339 (0.383)\tData 0.000 (0.069)\tLoss 0.1605 (0.1659)\tAcc@0 0.941 (0.940)\t\n",
      "Epoch: [3][40/81]\tTime 0.366 (0.373)\tData 0.000 (0.057)\tLoss 0.1258 (0.1551)\tAcc@0 0.957 (0.945)\t\n",
      "Epoch: [3][50/81]\tTime 0.347 (0.366)\tData 0.000 (0.049)\tLoss 0.1472 (0.1494)\tAcc@0 0.957 (0.947)\t\n",
      "Epoch: [3][60/81]\tTime 0.309 (0.363)\tData 0.000 (0.045)\tLoss 0.1836 (0.1507)\tAcc@0 0.914 (0.945)\t\n",
      "Epoch: [3][70/81]\tTime 0.369 (0.360)\tData 0.000 (0.041)\tLoss 0.2247 (0.1507)\tAcc@0 0.930 (0.945)\t\n",
      "Epoch: [3][80/81]\tTime 0.326 (0.357)\tData 0.000 (0.039)\tLoss 0.1126 (0.1507)\tAcc@0 0.960 (0.945)\t\n",
      "Test: [0/37]\tTime 1.212 (1.212)\tLoss 1.4080 (1.4080)\tAcc@0 0.430 (0.430)\t\n",
      "Test: [10/37]\tTime 0.121 (0.288)\tLoss 0.0128 (0.5223)\tAcc@0 1.000 (0.792)\t\n",
      "Test: [20/37]\tTime 0.436 (0.248)\tLoss 0.0116 (0.2801)\tAcc@0 1.000 (0.891)\t\n",
      "Test: [30/37]\tTime 0.137 (0.222)\tLoss 0.0129 (0.1937)\tAcc@0 0.996 (0.926)\t\n",
      " * Acc@0 0.937\n",
      "Epoch: [4][0/81]\tTime 1.351 (1.351)\tData 1.187 (1.187)\tLoss 0.1213 (0.1213)\tAcc@0 0.965 (0.965)\t\n",
      "Epoch: [4][10/81]\tTime 0.337 (0.435)\tData 0.000 (0.126)\tLoss 0.0933 (0.1262)\tAcc@0 0.969 (0.956)\t\n",
      "Epoch: [4][20/81]\tTime 0.347 (0.392)\tData 0.000 (0.076)\tLoss 0.1362 (0.1289)\tAcc@0 0.945 (0.956)\t\n",
      "Epoch: [4][30/81]\tTime 0.335 (0.376)\tData 0.000 (0.058)\tLoss 0.1733 (0.1280)\tAcc@0 0.938 (0.956)\t\n",
      "Epoch: [4][40/81]\tTime 0.340 (0.368)\tData 0.000 (0.049)\tLoss 0.1532 (0.1279)\tAcc@0 0.961 (0.955)\t\n",
      "Epoch: [4][50/81]\tTime 0.339 (0.363)\tData 0.000 (0.044)\tLoss 0.1622 (0.1276)\tAcc@0 0.938 (0.955)\t\n",
      "Epoch: [4][60/81]\tTime 0.364 (0.360)\tData 0.000 (0.040)\tLoss 0.1343 (0.1277)\tAcc@0 0.945 (0.955)\t\n",
      "Epoch: [4][70/81]\tTime 0.312 (0.357)\tData 0.000 (0.037)\tLoss 0.1324 (0.1271)\tAcc@0 0.941 (0.954)\t\n",
      "Epoch: [4][80/81]\tTime 0.316 (0.355)\tData 0.000 (0.035)\tLoss 0.1182 (0.1275)\tAcc@0 0.960 (0.954)\t\n",
      "Test: [0/37]\tTime 1.465 (1.465)\tLoss 0.4501 (0.4501)\tAcc@0 0.785 (0.785)\t\n",
      "Test: [10/37]\tTime 0.120 (0.286)\tLoss 0.1066 (0.2359)\tAcc@0 0.988 (0.911)\t\n",
      "Test: [20/37]\tTime 0.302 (0.241)\tLoss 0.1036 (0.1759)\tAcc@0 0.992 (0.950)\t\n",
      "Test: [30/37]\tTime 0.109 (0.220)\tLoss 0.0916 (0.1540)\tAcc@0 0.992 (0.963)\t\n",
      " * Acc@0 0.967\n",
      "Epoch: [5][0/81]\tTime 1.288 (1.288)\tData 1.140 (1.140)\tLoss 0.1651 (0.1651)\tAcc@0 0.949 (0.949)\t\n",
      "Epoch: [5][10/81]\tTime 0.336 (0.428)\tData 0.000 (0.141)\tLoss 0.1065 (0.1183)\tAcc@0 0.961 (0.957)\t\n",
      "Epoch: [5][20/81]\tTime 0.348 (0.387)\tData 0.000 (0.084)\tLoss 0.1315 (0.1299)\tAcc@0 0.957 (0.954)\t\n",
      "Epoch: [5][30/81]\tTime 0.335 (0.372)\tData 0.000 (0.064)\tLoss 0.1408 (0.1278)\tAcc@0 0.938 (0.955)\t\n",
      "Epoch: [5][40/81]\tTime 0.365 (0.365)\tData 0.000 (0.053)\tLoss 0.1420 (0.1303)\tAcc@0 0.953 (0.954)\t\n",
      "Epoch: [5][50/81]\tTime 0.342 (0.360)\tData 0.000 (0.046)\tLoss 0.1373 (0.1282)\tAcc@0 0.953 (0.955)\t\n",
      "Epoch: [5][60/81]\tTime 0.344 (0.357)\tData 0.000 (0.042)\tLoss 0.1146 (0.1278)\tAcc@0 0.953 (0.955)\t\n",
      "Epoch: [5][70/81]\tTime 0.339 (0.355)\tData 0.000 (0.039)\tLoss 0.0923 (0.1260)\tAcc@0 0.973 (0.956)\t\n",
      "Epoch: [5][80/81]\tTime 0.327 (0.353)\tData 0.000 (0.037)\tLoss 0.1572 (0.1275)\tAcc@0 0.965 (0.955)\t\n",
      "Test: [0/37]\tTime 1.314 (1.314)\tLoss 0.0455 (0.0455)\tAcc@0 0.977 (0.977)\t\n",
      "Test: [10/37]\tTime 0.134 (0.304)\tLoss 3.3773 (2.2511)\tAcc@0 0.492 (0.660)\t\n",
      "Test: [20/37]\tTime 0.485 (0.253)\tLoss 2.8890 (2.7702)\tAcc@0 0.492 (0.569)\t\n",
      "Test: [30/37]\tTime 0.112 (0.229)\tLoss 3.2360 (2.9145)\tAcc@0 0.500 (0.545)\t\n",
      " * Acc@0 0.534\n",
      "Epoch: [6][0/81]\tTime 1.478 (1.478)\tData 1.336 (1.336)\tLoss 0.0806 (0.0806)\tAcc@0 0.977 (0.977)\t\n",
      "Epoch: [6][10/81]\tTime 0.337 (0.442)\tData 0.000 (0.140)\tLoss 0.0900 (0.1235)\tAcc@0 0.961 (0.951)\t\n",
      "Epoch: [6][20/81]\tTime 0.353 (0.394)\tData 0.000 (0.084)\tLoss 0.0734 (0.1180)\tAcc@0 0.973 (0.957)\t\n",
      "Epoch: [6][30/81]\tTime 0.345 (0.377)\tData 0.000 (0.063)\tLoss 0.1046 (0.1164)\tAcc@0 0.961 (0.958)\t\n",
      "Epoch: [6][40/81]\tTime 0.340 (0.368)\tData 0.000 (0.053)\tLoss 0.0894 (0.1130)\tAcc@0 0.969 (0.960)\t\n",
      "Epoch: [6][50/81]\tTime 0.339 (0.362)\tData 0.000 (0.046)\tLoss 0.1323 (0.1154)\tAcc@0 0.949 (0.959)\t\n",
      "Epoch: [6][60/81]\tTime 0.345 (0.359)\tData 0.000 (0.042)\tLoss 0.1680 (0.1164)\tAcc@0 0.922 (0.958)\t\n",
      "Epoch: [6][70/81]\tTime 0.351 (0.357)\tData 0.000 (0.039)\tLoss 0.1426 (0.1163)\tAcc@0 0.945 (0.958)\t\n",
      "Epoch: [6][80/81]\tTime 0.320 (0.354)\tData 0.000 (0.037)\tLoss 0.0778 (0.1187)\tAcc@0 0.970 (0.958)\t\n",
      "Test: [0/37]\tTime 1.239 (1.239)\tLoss 1.7851 (1.7851)\tAcc@0 0.320 (0.320)\t\n",
      "Test: [10/37]\tTime 0.109 (0.279)\tLoss 0.0489 (0.7020)\tAcc@0 0.988 (0.744)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [20/37]\tTime 0.308 (0.230)\tLoss 0.0392 (0.3865)\tAcc@0 0.992 (0.864)\t\n",
      "Test: [30/37]\tTime 0.236 (0.218)\tLoss 0.0741 (0.2765)\tAcc@0 0.973 (0.905)\t\n",
      " * Acc@0 0.919\n",
      "Epoch: [7][0/81]\tTime 1.583 (1.583)\tData 1.455 (1.455)\tLoss 0.1454 (0.1454)\tAcc@0 0.941 (0.941)\t\n",
      "Epoch: [7][10/81]\tTime 0.339 (0.454)\tData 0.000 (0.151)\tLoss 0.0894 (0.1331)\tAcc@0 0.961 (0.952)\t\n",
      "Epoch: [7][20/81]\tTime 0.346 (0.400)\tData 0.000 (0.089)\tLoss 0.1294 (0.1289)\tAcc@0 0.949 (0.953)\t\n",
      "Epoch: [7][30/81]\tTime 0.332 (0.381)\tData 0.000 (0.067)\tLoss 0.1148 (0.1196)\tAcc@0 0.965 (0.957)\t\n",
      "Epoch: [7][40/81]\tTime 0.339 (0.371)\tData 0.000 (0.056)\tLoss 0.1384 (0.1221)\tAcc@0 0.945 (0.956)\t\n",
      "Epoch: [7][50/81]\tTime 0.338 (0.366)\tData 0.000 (0.049)\tLoss 0.0843 (0.1173)\tAcc@0 0.977 (0.958)\t\n"
     ]
    }
   ],
   "source": [
    "best_acc1 = 0\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "for epoch in range(epochs):\n",
    "    # train for one epoch\n",
    "    train(train_loader, cnn, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, cnn, criterion, epoch)\n",
    "\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "    if save:\n",
    "        save_dir = 'checkpoint_{}_{}.pth.tar'.format(arch, suffix)\n",
    "        if (epoch+1)%save_epoch == 0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'acc1': acc1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best, save_dir)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def accuracy_VOC2012(output, target):\n",
    "    with torch.no_grad():\n",
    "        batch_size = target.size(0)\n",
    "        accur = output.gt(0.).long().eq(target.long()).float().mean()\n",
    "        res = []\n",
    "        res.append(accur)\n",
    "        return res\n",
    "\n",
    "class CNN():\n",
    "    def __init__(self, params = {}):\n",
    "        self.batch_size = self.set_params(256, 'batch_size', params)\n",
    "        self.workers = self.set_params(4, 'workers', params)\n",
    "        self.gpu = self.set_params(0, 'gpu', params)\n",
    "        self.arch = self.set_params('resnet18', 'arch', params)\n",
    "        self.optim = self.set_params('Adam', 'optim', params)\n",
    "        self.lr = self.set_params(0.1, 'lr', params)\n",
    "        self.step_size = self.set_params(30, 'step_size', params)\n",
    "        self.gamma = self.set_params(0.1, 'gamma', params)\n",
    "        self.weight_decay = self.set_params(1e-4, 'weight_decay', params)\n",
    "        self.momentum = self.set_params(0.9, 'momentum', params)\n",
    "        self.epochs = self.set_params(90, 'epochs', params)\n",
    "        self.save_epoch = self.set_params(50, 'save_epoch', params)\n",
    "        self.save = self.set_params(False, 'save', params)\n",
    "        self.suffix = self.set_params('', 'suffix', params)\n",
    "        self.print_freq = self.set_params(10, 'print_freq', params)\n",
    "        \n",
    "        self.cnn = models.__dict__[self.arch](num_classes=1)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        if self.gpu is not None:\n",
    "            self.cnn.cuda(self.gpu)\n",
    "            self.criterion.cuda(self.gpu)        \n",
    "        \n",
    "        if self.optim == 'SGD':\n",
    "            self.optimizer = torch.optim.SGD(self.cnn.parameters(), self.lr, \n",
    "                                             momentum = self.momentum,\n",
    "                                             weight_decay=self.weight_decay)\n",
    "        elif self.optim == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.cnn.parameters(), self.lr, \n",
    "                                              weight_decay = self.weight_decay)\n",
    "        \n",
    "        normalize = T.Normalize(mean = [0.5116, 0.4200, 0.3651],\n",
    "                         std = [0.2655, 0.2430, 0.2420])\n",
    "        self.img_trans = T.Compose([\n",
    "                      T.Resize((96,96)),\n",
    "                      T.ToTensor(),\n",
    "                      normalize\n",
    "                  ])\n",
    "        \n",
    "    def set_params(self, default, label, params):\n",
    "        return default if label not in params.keys() else params[label]\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top0 = AverageMeter()\n",
    "\n",
    "        # switch to train mode\n",
    "        self.cnn.train()\n",
    "        \n",
    "#         enum = tqdm.tqdm(enumerate(self.train_loader),\n",
    "#                          total= len(self.train_loader),\n",
    "#                         desc='Train: Loss {loss.val:.4f} ({loss.avg:.4f}) '\n",
    "#                              'Acc@0 {top0.val:.3f} ({top0.avg:.3f})'\n",
    "#                          .format(loss=losses, top0=top0))\n",
    "        \n",
    "        for i, (input, target) in enumerate(self.train_loader):\n",
    "\n",
    "            if self.gpu is not None:\n",
    "                input = input.cuda(self.gpu, non_blocking=True)\n",
    "                target = target.float().unsqueeze(-1).cuda(self.gpu, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = self.cnn(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            acc = accuracy_VOC2012(output, target)\n",
    "\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top0.update(acc[0], input.size(0))\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "#             if (i+1) % self.print_freq == 0:\n",
    "#                 enum.set_description('Train: Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                                      'Acc@0 {top0.val:.3f} ({top0.avg:.3f})'\n",
    "#                                      .format(loss=losses, top0=top0))\n",
    "        return losses.avg, top0.avg\n",
    "\n",
    "    def validate(self):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top0 = AverageMeter()\n",
    "\n",
    "        # switch to evaluate mode\n",
    "        self.cnn.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "#             enum = tqdm.tqdm(enumerate(self.val_loader), \n",
    "#                       total=len(self.val_loader), \n",
    "#                       desc='Test: Loss {loss.val:.4f} ({loss.avg:.4f}) '\n",
    "#                           'Acc@0 {top0.val:.3f} ({top0.avg:.3f})'\n",
    "#                       .format( loss=losses, top0=top0))\n",
    "            for i, (input, target) in enumerate(self.val_loader):\n",
    "                if self.gpu is not None:\n",
    "                    input = input.cuda(self.gpu, non_blocking=True)\n",
    "                    target = target.float().unsqueeze(-1).cuda(self.gpu, non_blocking=True)\n",
    "\n",
    "                # compute output\n",
    "                output = self.cnn(input)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                acc = accuracy_VOC2012(output, target)\n",
    "                losses.update(loss.item(), input.size(0))\n",
    "                top0.update(acc[0], input.size(0))\n",
    "#                 if (i+1) % self.print_freq == 0:\n",
    "#                     enum.set_description('Test: Loss {loss.val:.4f} ({loss.avg:.4f}) '\n",
    "#                                          'Acc@0 {top0.val:.3f} ({top0.avg:.3f})'\n",
    "#                                     .format(loss=losses, top0=top0))\n",
    "        return top0.avg\n",
    "\n",
    "    def save_checkpoint(self, state, is_best, filename='checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(filename, 'model_best_{}_{}.pth.tar'\n",
    "                            .format(self.arch, self.suffix))\n",
    "    \n",
    "    def load_checkpoint(self, resume):\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(args.resume, map_location=torch.device(\"cuda:{}\".format(self.gpu)))\n",
    "            self.cnn.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {}) as Teacher!\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "            del checkpoint\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "            return\n",
    "    \n",
    "    def fit(self):\n",
    "        best_acc1 = 0\n",
    "        acc1 = 0\n",
    "        acc1_test = 0\n",
    "        scheduler = lr_scheduler.StepLR(self.optimizer, \n",
    "                                        step_size=self.step_size, \n",
    "                                        gamma=self.gamma)\n",
    "        epochs = tqdm.tqdm(range(self.epochs), \n",
    "                               desc='Train Acc {:.4f} Test Acc {:.4f} ({:.4f}) Epoch'.format(acc1_test, best_acc1, acc1))\n",
    "        for epoch in epochs:\n",
    "            # train for one epoch\n",
    "            _, acc1_test = self.train(epoch)\n",
    "\n",
    "            # evaluate on validation set\n",
    "            acc1 = self.validate()\n",
    "            \n",
    "            # remember best acc@1 and save checkpoint\n",
    "            is_best = acc1 > best_acc1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            if self.save:\n",
    "                save_dir = 'checkpoint_{}_{}.pth.tar'.format(self.arch, self.suffix)\n",
    "                if (epoch+1)%self.save_epoch == 0:\n",
    "                    self.save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'arch': self.arch,\n",
    "                        'state_dict': self.cnn.state_dict(),\n",
    "                        'best_acc1': best_acc1,\n",
    "                        'acc1': acc1,\n",
    "                        'optimizer' : self.optimizer.state_dict(),\n",
    "                    }, is_best, save_dir)\n",
    "                elif is_best:\n",
    "                    self.save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'arch': self.arch,\n",
    "                        'state_dict': self.cnn.state_dict(),\n",
    "                        'best_acc1': best_acc1,\n",
    "                        'acc1': acc1,\n",
    "                        'optimizer' : self.optimizer.state_dict(),\n",
    "                    }, is_best, save_dir)\n",
    "            epochs.set_description_str('Train Acc {:.4f} Test Acc {:.4f} ({:.4f})'\n",
    "                                       .format(acc1_test, best_acc1, acc1))\n",
    "            scheduler.step()\n",
    "            \n",
    "    def set_dataset(self, db_train, db_val):\n",
    "        self.train_loader = DataLoader(db_train, batch_size=self.batch_size, shuffle=True,\n",
    "                                 num_workers=self.workers, pin_memory=True)\n",
    "        self.val_loader = DataLoader(db_val, batch_size=self.batch_size, shuffle=False,\n",
    "                               num_workers=self.workers, pin_memory=True)\n",
    "    \n",
    "    def get_score(self, X):\n",
    "        self.cnn.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.gpu is not None:\n",
    "                input = input.cuda(self.gpu, non_blocking=True)\n",
    "                target = target.float().unsqueeze(-1).cuda(self.gpu, non_blocking=True)\n",
    "            output = self.cnn(input)\n",
    "\n",
    "        return output.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = FDDB('train_list.npy', \n",
    "                transform=T.Compose([\n",
    "                    T.RandomHorizontalFlip(),\n",
    "                    M.img_trans\n",
    "                ]),\n",
    "                zero_one=True\n",
    "               )\n",
    "db_val = FDDB('val_list.npy', \n",
    "              transform=M.img_trans,\n",
    "              zero_one=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.set_dataset(db_train, db_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/200 [00:11<39:17, 11.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 2/200 [00:23<39:02, 11.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 3/200 [00:35<38:58, 11.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 4/200 [00:47<38:57, 11.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 5/200 [00:59<38:38, 11.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 6/200 [01:11<38:24, 11.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 7/200 [01:23<38:16, 11.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 8/200 [01:35<38:04, 11.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 9/200 [01:47<37:49, 11.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e71a7a3c54d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-3d787a2c67f5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-3d787a2c67f5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;31m#             if (i+1) % self.print_freq == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m#                 enum.set_description('Train: Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "M.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
